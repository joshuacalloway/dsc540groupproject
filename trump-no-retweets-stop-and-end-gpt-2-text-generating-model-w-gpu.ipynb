{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trump No Retweets stop and end GPT-2 Text-Generating Model w/ GPU",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/zitterbewegung/bf7fc4a7921f839b370e63a5dda92c1d/trump-no-retweets-stop-and-end-gpt-2-text-generating-model-w-gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: May 19th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple).\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory now uses an Nvidia T4 GPU, which is slightly faster than the old Nvidia K80 GPU for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "b27d3301-4e03-48ca-f821-0969c80b6a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug  5 22:58:56 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    15W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are two sizes of GPT-2:\n",
        "\n",
        "* `117M` (default): the \"small\" model, 500MB on disk.\n",
        "* `345M`: the \"medium\" model, 1.5GB on disk.\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "93aa03d0-63c6-41a3-c1be-66a17e2b462d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"345M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 219Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 97.2Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 264Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:08, 164Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 239Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 88.8Mit/s]                                                \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 147Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "65457c66-875d-4679-ca0f-bd608e8c9f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"/content/trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-as-link.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "2262016c-568b-4c95-d3b7-cc39f3c44a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='345M',\n",
        "              steps=4000,\n",
        "              restore_from='latest',\n",
        "              run_name='trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt',\n",
        "              print_every=100,\n",
        "              sample_every=200,\n",
        "              save_every=1000,\n",
        "              overwrite=True\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0713 11:53:41.719954 139994155657088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:164: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0713 11:53:41.723483 139994155657088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0713 11:53:56.185151 139994155657088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:71: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0713 11:53:56.209380 139994155657088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0713 11:53:56.213638 139994155657088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0713 11:53:56.232842 139994155657088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:191: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0713 11:54:16.247142 139994155657088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:198: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0713 11:54:16.250902 139994155657088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:200: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0713 11:54:16.256154 139994155657088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:202: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0713 11:54:27.649991 139994155657088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/345M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:07<00:00,  7.22s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1104119 tokens\n",
            "Training...\n",
            "Saving checkpoint/trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt/model-0\n",
            "[100 | 325.31] loss=2.03 avg=2.03\n",
            "[200 | 638.27] loss=1.65 avg=1.84\n",
            "======== SAMPLE 1 ========\n",
            " who are at odds with Trump. He is an embarrassment to our country. It does not make any sense!<|endoftext|>\n",
            "<|startoftext|>With so much corruption in Washington we do not know how much of the enormous &amp; very high cost of healthcare is going to be paid for by the government. I will fix!<|endoftext|>\n",
            "<|startoftext|>There was a big win by the Democrats in the Senate. The Freedom Caucus gave everything for the good of the Freedom Caucus. No way to beat them in the Senate.<|endoftext|>\n",
            "<|startoftext|>Thank you Virginia for your hard work. A great and special day in Richmond!<|endoftext|>\n",
            "<|startoftext|>Thank you for the kind words and kind words from both sides of the debate in the debate last night. Enjoy!<|endoftext|>\n",
            "<|startoftext|>If ever there was a debate about ObamaCare the ObamaCare repeal would probably have been much stronger and faster than the ObamaCare debate today. I would have easily defeated them both!<|endoftext|>\n",
            "<|startoftext|>Congressman Duncan Hunter (R) of the American People and many others did a fantastic job in negotiating with our great &amp; great Military - the finest, most brave and most respected in our Country!<|endoftext|>\n",
            "<|startoftext|>I heard it was the stupidest thing I ever hear about my opponent on Fox. He would have killed ObamaCare and totally failed. He is a lightweight!<|endoftext|>\n",
            "<|startoftext|>I will be discussing healthcare and taxes with Senators Lindsey Graham Lindsey Graham and Richard Shelby Lindsey Graham and others at the beginning of next week. Please have patience and give us flexibility to work with the GOP. Tax reform is all we need!<|endoftext|>\n",
            "<|startoftext|>@JebBush did exactly what he was hired to do before. He is a nice guy who knew my brother had no money!<|endoftext|>\n",
            "<|startoftext|>When I asked Carly about her book \"The America We Deserve\" she was unable to give a straight answer. She said she could not be so blunt about it because no one would trust me with it. So dishonest!<|endoftext|>\n",
            "<|startoftext|>Why doesn't @JohnDMcCain know how Obamacare goes in the House? Why hasn't he gone for it? I would repeal it and give our country new health care<|endoftext|>\n",
            "<|startoftext|>@taweecn @JebBush @SarahPalinUSA  Let me have your money @JebBush's $2.5 million of your campaign contribution. URL<|endoftext|>\n",
            "<|startoftext|>Thank you for the kind words from both sides of the debate last night. Enjoy!<|endoftext|>\n",
            "<|startoftext|>“Make no mistake about it ObamaCare is not working and it is being replaced. ObamaCare is not dying &amp; will continue to be replaced by private insurance.'' @LouDobbs<|endoftext|>\n",
            "<|startoftext|>I will be discussing healthcare and taxes with Senators Lindsey Graham Lindsey Graham &amp; others at the beginning of next week. Please have patience and give us flexibility to work with the GOP. Tax reform is all we need!<|endoftext|>\n",
            "<|startoftext|>@Ralphandale New book coming out with the @GOP Congress and I am the only one who could negotiate with Republicans- and ObamaCare is dead! @foxandfriends<|endoftext|>\n",
            "<|startoftext|>@rjr12  I will be in California on Friday - at the event of an indictment of John Brennan (CIA Director)!<|endoftext|>\n",
            "<|startoftext|>@lunarjeff  I will be in California!<|endoftext|>\n",
            "<|startoftext|>@greta17  Don't miss it!<|endoftext|>\n",
            "<|startoftext|>@jesus_noto  I will be in the Great State of Florida on Election Day. I will be on @oreillyfactor with my wife Melania &amp; Barron!<|endoftext|>\n",
            "<|startoftext|>Join me in the Great State of Florida this Tuesday September 6th. Will be on @oreillyfactor with my wife Melania &amp; Barron!<|endoftext|>\n",
            "<|startoftext|>Join me in the Great State of Florida this Tuesday September 6th.\n",
            "\n",
            "[300 | 992.62] loss=1.88 avg=1.85\n",
            "[400 | 1306.34] loss=1.99 avg=1.89\n",
            "======== SAMPLE 1 ========\n",
            " tonight!  #MAGA”@seanspicer<|endoftext|>\n",
            "<|startoftext|>@bobbyjindal  Yes please and thanks!<|endoftext|>\n",
            "<|startoftext|>@johngreen  Thank you.<|endoftext|>\n",
            "<|startoftext|>@johnnyjkrn  Thanks.<|endoftext|>\n",
            "<|startoftext|>@J_TrevorR @KPX3P  @johnnykrn and thanks!<|endoftext|>\n",
            "<|startoftext|>@gavinmdawkins @theblaze @TigerWoods OK so funny.<|endoftext|>\n",
            "<|startoftext|>@JTrevorR  Thank you.<|endoftext|>\n",
            "<|startoftext|>@KPX3P  Thanks.<|endoftext|>\n",
            "<|startoftext|>@Johnnykrn thanks.<|endoftext|>\n",
            "<|startoftext|>I love @RNC.  They are making the Republican Party great again--be a part of it!<|endoftext|>\n",
            "<|startoftext|>Congratulations @RNC on holding the first ever RNC convention in Buffalo New York.<|endoftext|>\n",
            "<|startoftext|>@RNC convention will be a great success and show that the R.N.C., at its finest can succeed.<|endoftext|>\n",
            "<|startoftext|>@JTrevorR Thanks.<|endoftext|>\n",
            "<|startoftext|>@TigerWoods @TigerWoods GREAT!<|endoftext|>\n",
            "<|startoftext|>\"@realDonaldTrump in our society these days the real problem is that people are too stupid to recognize it.\"\" @TrumpWaikiki<|endoftext|>\n",
            "<|startoftext|>@tigerwoods  Thanks.<|endoftext|>\n",
            "<|startoftext|>\"Trump to Win Major Award at @CelebApprentice \"Celebrity Apprentice\" for Season 2\"Celebrity Apprentice\" URL<|endoftext|>\n",
            "<|startoftext|>.@TrumpNashua will be a great addition to the community. We can't allow it to deteriorate!\" We must #MakeAmericaGreatAgain<|endoftext|>\n",
            "<|startoftext|>.@MittRomney and @BarackObama's plan to slash taxes and lower regulations will get a great response in November.<|endoftext|>\n",
            "<|startoftext|>@MittRomney in his press conference in December 2012 asked 'Who? Who? Why?'.<|endoftext|>\n",
            "<|startoftext|>\"@TrumpNew York is located in one of the most historic locations in the world--all 3 towers combined have an astounding 13000' @Trump\" URL<|endoftext|>\n",
            "<|startoftext|>@Trump New York is the best in the game at @TrumpDept URL<|endoftext|>\n",
            "<|startoftext|>@Trump NEW YORK is the most beautiful golf course in Manhattan. @BarackObama's approval ratings are at their worst in over 40 years.<|endoftext|>\n",
            "<|startoftext|>I will sign a new FAST &amp; SECURE TAX BILL in the New Year--which includes the biggest cut in business taxes in history for American consumers.<|endoftext|>\n",
            "<|startoftext|>I will be visiting New York on August 5th to meet with the World-Class Designers of @TrumpInt'l Hotel &amp; Tower.<|endoftext|>\n",
            "<|startoftext|>@Trump Int'l Hotel &amp; Tower has the best views of any hotel in the world URL<|endoftext|>\n",
            "<|startoftext|>Thank you @IvankaTrump for your leadership on #KellyannePolls. Keep up the great work! #MAGA<|endoftext|>\n",
            "<|startoftext|>It`s #Trump2016!#KellyannePolls URL<|endoftext|>\n",
            "<|startoftext|>If @BarackObama wants to sign new fast or quick-and-easy budget it must be signed now. @JAMMELOW<|endoftext|>\n",
            "<|startoftext|>@DanaPerino @dana_perino You will be very disappointed.<|endoftext|>\n",
            "<|startoftext|>@jrhodes @realDonaldTrump great!!<\n",
            "\n",
            "[500 | 1657.50] loss=2.05 avg=1.92\n",
            "[600 | 1971.40] loss=1.60 avg=1.87\n",
            "======== SAMPLE 1 ========\n",
            "text|>Via BreitbartNews by @BreitbartNews  \"President Donald Trump’s White House Staff Picks: The 50 Greatest Trump Trump Ideas of All Time\" URL<|endoftext|>\n",
            "<|startoftext|>Our hearts go out to the victims of this terrible and horrible tragedy in St. Cloud Minnesota as we reflect on its horrors being called a terrorist act?<|endoftext|>\n",
            "<|startoftext|>A day after Donald Trump announced that he was stepping down from the Apprentice and moving to Trump Tower his show is struggling to be heard by viewers?<|endoftext|>\n",
            "<|startoftext|>When @BretBaier said he “would be giving up on the Trump show” I said it was brilliant!<|endoftext|>\n",
            "<|startoftext|>“President Trump's Most Overlooked Celebrity Quotes: “If you can’t afford anything you should buy it.”” URL\" The Forbes List URL<|endoftext|>\n",
            "<|startoftext|>The Washington Post just named Melania a 'Wedding of the Year' URL  I know why!<|endoftext|>\n",
            "<|startoftext|>Donald Trump’s Trump Doral course at Miami’s prestigious Trump International Golf Links was voted one of the best in all of golf golf.   The golf course looks great!<|endoftext|>\n",
            "<|startoftext|>Via @TrumpSays: \"\"I always enjoy being an entertainer - especially in the world of entertainment.  Making the world a more beautiful place to live is my love.\"<|endoftext|>\n",
            "<|startoftext|>#trumpvlog  URL<|endoftext|>\n",
            "<|startoftext|>I am honored to share my thoughts and memories as host of the Trump Tower Dinner on January 8.   A real joy to be honored at the prestigious Trump International Golf Links in Miami!<|endoftext|>\n",
            "<|startoftext|>.@IvankaTrump’s first Thanksgiving dinner as president- URL   @IvankaTrump’s new @IvankatDoral golf course with a view of the Miami skyline URL<|endoftext|>\n",
            "<|startoftext|>New @NRO column: #trumpvlog  @BretBaier: A Very Trump Thing—a success story of a big man's career. URL<|endoftext|>\n",
            "<|startoftext|>Trump: @BarackObama's foreign policy has 'been a disaster' URL  URL<|endoftext|>\n",
            "<|startoftext|>This is a success story of a big man's success: #trumpvlog #BarackObama’s failed policy of regime change URL<|endoftext|>\n",
            "<|startoftext|>From my @NRO interview discussing my new book Trump and the Great Art of The Deal \"the best of the year.\" URL<|endoftext|>\n",
            "<|startoftext|>Just left a great event packed restaurant in Trump Tower- URL Thanks to @nytimes for naming it - I'm glad!<|endoftext|>\n",
            "<|startoftext|>On December 15th I celebrate the grand opening of Trump World Tower- URL by @DonaldJTrump   URL<|endoftext|>\n",
            "<|startoftext|>#trumpvlog What are the great achievements of the Trump family? URL #trumpfamily #TrumpCeremony<|endoftext|>\n",
            "<|startoftext|>If Obama is impeached he will be the first President in recorded history—and we'll never be able to build the New Deal<|endoftext|>\n",
            "<|startoftext|>Via @MailOnline  #TrumpIsNotJuda:  Trump praises Jewish voters” URL \"Trump\" URL<|endoftext|>\n",
            "<|startoftext|>Just left a fantastic event—my Twitter followers will be big!<|endoftext|>\n",
            "<|startoftext|>Wow!#trumpvlog  URL   Thanks to @WAPR for giving me my first chance URL<|endoftext|>\n",
            "<|startoftext|>In case you think I forgot- it was @WAPR which I love which named #trumpvlog? URL<|endoftext|>\n",
            "<|startoftext|>@VanityFair has just stated that @TrumpTowerNY \"is one of the best built offices in the country\" URL  URL<|endoftext|>\n",
            "<|startoftext|>From @BreitbartNews: 'Trump's\n",
            "\n",
            "[700 | 2322.86] loss=2.08 avg=1.90\n",
            "[800 | 2636.92] loss=1.33 avg=1.82\n",
            "======== SAMPLE 1 ========\n",
            " get that money right back into U.S.A. - A great moment for America.<|endoftext|>\n",
            "<|startoftext|>@BretBaier @EricTrumpFdn @TrumpDoral  Great thanks!<|endoftext|>\n",
            "<|startoftext|>With @MELANIATRUMP @ApprenticeNBC now on @nbc. Watch your little friend grow up fast!<|endoftext|>\n",
            "<|startoftext|>@mrslumichill  What the hell you doing!? You had me go through many painful tests before you went through with you--fraud!<|endoftext|>\n",
            "<|startoftext|>@mrslumichill   Your father is good at his job. You have an incredible gift. You are a wonderful lad.<|endoftext|>\n",
            "<|startoftext|>@mrslumichill  Thanks.<|endoftext|>\n",
            "<|startoftext|>@mrslumichill  Great.<|endoftext|>\n",
            "<|startoftext|>@mrslumichill  It was beautiful<|endoftext|>\n",
            "<|startoftext|>@mrslumichill  I am!<|endoftext|>\n",
            "<|startoftext|>@MRSLUMICHILL   Nice.<|endoftext|>\n",
            "<|startoftext|>.@TrumpNationalGolfDC in Northern Virginia is a 5 star resort where golfers are treated to amazing courses &amp; the finest hospitality available.<|endoftext|>\n",
            "<|startoftext|>@KelseyMacFarlane is a total mess. The father of Sean Patrick has since apologized to the family--the daughter said nothing. There is no way this is possible.<|endoftext|>\n",
            "<|startoftext|>@RachaelArseny                             \"They say you'll find them anywhere. They're really true—thanks\"                               \"@TrumpDoral is such a unique location in the Miami area with the finest views in the world\"                          \"Amazing!\"<|endoftext|>\n",
            "<|startoftext|>@piersmorgan           \"@realDonaldTrump A great day. Thank you for the wonderful memory of the Great Danforth.\" Thank you.<|endoftext|>\n",
            "<|startoftext|>@TrumpDoral's 18 holes of the course are world record breaking. There are more incredible holes in that course than you are likely to see with any course!<|endoftext|>\n",
            "<|startoftext|>My friends from @TrumpTowerNY's 3rd floor suite for @TrumpDoral is located at the top of the building (a great honor). It is a beautiful space and top for a good cause<|endoftext|>\n",
            "<|startoftext|>@TrumpNewYork                @TrumpGuilfoyle                         \"@EricTrump: @ApprenticeNBC has produced some of the greatest people and shows in all of television....and we were lucky,\" A.<|endoftext|>\n",
            "<|startoftext|>@EricTrump  Great job--you'll never go back|<|endoftext|>\n",
            "<|startoftext|>I had a brilliant day last Monday before the final two contestants in the Apprentice 1.<|endoftext|>\n",
            "<|startoftext|>The most successful thing Donald Trump has done so far as host of The Apprentice is to produce the greatest reality show ever.<|endoftext|>\n",
            "<|startoftext|>My great friend from the first season of Celebrity Apprentice John Wayne once again takes to the airwaves to celebrate his 50 anniversary.<|endoftext|>\n",
            "<|startoftext|>@mrslumichill  I can't believe I am allowed to say that.<|endoftext|>\n",
            "<|startoftext|>Do you think the Obama Administration will shut down the embassy in Egypt and do the right thing?<|endoftext|>\n",
            "<|startoftext|>.@TrumpFdn is run by the same family and is\n",
            "\n",
            "[900 | 2987.67] loss=1.73 avg=1.81\n",
            "[1000 | 3301.20] loss=1.69 avg=1.80\n",
            "Saving checkpoint/trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt/model-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0713 12:50:01.319637 139994155657088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "======== SAMPLE 1 ========\n",
            ">A lot of great and loyal folks out there who are going to help get me reelected. Many of those people know me personally from my various ventures. This is a total REPUBLICAN BLOCK. They will have their own ideas and will be very vocal!<|endoftext|>\n",
            "<|startoftext|>My great honor to hold this year’s “Statesman of the Year” Inaugural Ball at the Old Post Office Pavilion in Washington D.C. URL #MAGA URL<|endoftext|>\n",
            "<|startoftext|>Join me in Phoenix tomorrow at 11:30am! My wonderful wife is honored to be hosting the 2018 @USOpen!URL<|endoftext|>\n",
            "<|startoftext|>Just landed in Phoenix a very special evening - a wonderful honor! Thank you Phoenix!#Trump2016 URL<|endoftext|>\n",
            "<|startoftext|>I will be making a major speech in Phoenix Ohio at 11:30am - it will be fantastic! This is a MOVEMENT like no other. Thank you! #MAGA URL<|endoftext|>\n",
            "<|startoftext|>I will be in Phoenix at the Old Post Office Pavilion in D.C. next Saturday at the 11:30am start time. Great crowd! #Trump2016 URL<|endoftext|>\n",
            "<|startoftext|>I will be doing the following this evening in Mesa Arizona: A) A rally in Mesa at 8:30pm - will be incredible. B) Good news conference in Mesa!<|endoftext|>\n",
            "<|startoftext|>My @foxandfriends interview from just before the #MissUniverse contestants announced the Miss Universe 2018 contestants URL<|endoftext|>\n",
            "<|startoftext|>My wife Melania loves talking about her amazing husband I think she almost says “I don't know why I don't know him anymore. He is my favorite and it’s my honor to spend so many hours with him.”<|endoftext|>\n",
            "<|startoftext|>Will be speaking in Phoenix Arizona tomorrow. Tickets at 3:00pm. See you there! #Trump2016 URL<|endoftext|>\n",
            "<|startoftext|>Thank you for all of the support Georgia New York and Miss Utah! We will be headed to Arizona and Arizona by the millions for an even bigger crowd than usual! URL<|endoftext|>\n",
            "<|startoftext|>.@MissUSA Pageant just won the Miss Universe Pageant crown in Washington D.C. A big and very proud victory for the U.S. A huge and very good show.<|endoftext|>\n",
            "<|startoftext|>If anyone thought the Miss Universe contestants were too smart and beautiful for Miss Pennsylvania to be on the show they were wrong! She is a total winner! #MissUniverse<|endoftext|>\n",
            "<|startoftext|>Thank you New York! #Trump2016URL<|endoftext|>\n",
            "<|startoftext|>Thank you for being an incredible supporter! #Trump2016Tickets: URL URL<|endoftext|>\n",
            "<|startoftext|>#Miss Universe Pageant #MissUnited States contestants were incredible! URL<|endoftext|>\n",
            "<|startoftext|>Miss Arizona was fantastic! #MissUSA Pageant Miss Florida was great! #MissUniverse 2016 Miss Pennsylvania was amazing!<|endoftext|>\n",
            "<|startoftext|>Heading to Miss Utah. We will be going to Las Vegas the same afternoon! #MissUniverse<|endoftext|>\n",
            "<|startoftext|>Miss Arizona was great! #MissUSA Pageant #MissPennsylvania was wonderful. #MissUniverse<|endoftext|>\n",
            "<|startoftext|>Miss Texas was great! #MissUSA Pageant #MissUniverse was amazing to see!<|endoftext|>\n",
            "<|startoftext|>Miss Illinois was amazing! #MissUSA PageantMiss Georgia was so beautiful! #MissUniverse<|endoftext|>\n",
            "<|startoftext|>Miss Minnesota was GREAT! #MissUSA Viewer's Choice #MissUSA #MissSouth Carolina Miss Montana Nevada and all of Arkansas were wonderful. See it tonight!<|endoftext|>\n",
            "<|startoftext|>We will be going to Las Vegas Nevada on tonight for the Miss Universe pageant. Will be fantastic.<|endoftext|>\n",
            "<|startoftext|>Thank you @MissUSA @MissUtah @MissArizona &amp; @MissArizonaUSA. Thank you for your beautiful comments on the Miss Universe Pageant tonight! I couldn\n",
            "\n",
            "[1100 | 3664.07] loss=1.82 avg=1.80\n",
            "[1200 | 3977.67] loss=1.80 avg=1.80\n",
            "======== SAMPLE 1 ========\n",
            " wishes to be here with Godmy friend the great Billy Graham -- would be honoured to do so. Love what you do @BillyGraham<|endoftext|>\n",
            "<|startoftext|>I am honored to host the #MissUniverse Pageant next week on NBC with its #CNBC Live audience. A special treat for everyone!<|endoftext|>\n",
            "<|startoftext|>#ICYMI- @jimmyfallon returns to the show. Great news.  Jim is wonderful!<|endoftext|>\n",
            "<|startoftext|>Congratulations to @BillKristol on his new book \"Inconvenient Truth.\" Wonderful book!<|endoftext|>\n",
            "<|startoftext|>I have known Billy Graham—not only did I know him—but I knew how much he loved me. I was just not there for him. But now that I am… URL<|endoftext|>\n",
            "<|startoftext|>My @GolfMagazine cover feature is on me with my beautiful family.  And there is more to come (cont) URL<|endoftext|>\n",
            "<|startoftext|>.@GolfMagazine just named Bill Kristol the best of the last 5 years URL #Trump2016<|endoftext|>\n",
            "<|startoftext|>Now @BarbaraJWalters is telling her fans that I am the ONLY one who can save Social Security and Medicare<|endoftext|>\n",
            "<|startoftext|>I hope you can go to the polls tomorrow and vote for me. I will fix your healthcare problems!<|endoftext|>\n",
            "<|startoftext|>Hillary Clinton said that she can't go on because she is on an airplane—i.e. she can't fly!<|endoftext|>\n",
            "<|startoftext|>Hillary Clinton said that she's \"not fit to lead this country\" —i.e. she can't go on a plane.<|endoftext|>\n",
            "<|startoftext|>Wow I got a lot of nice reviews from across the world after my speech-and nowhere so much as @GolfMagazine!<|endoftext|>\n",
            "<|startoftext|>Hillary Clinton can't go on board an Amtrak. She's weak and old and can't even run a normal country.<|endoftext|>\n",
            "<|startoftext|>Hillary Clinton said that she is \"not fit to lead this country\"—i.e. she can't go run a normal country.<|endoftext|>\n",
            "<|startoftext|>Hillary Clinton cannot go on an Amtrak. She's weak and old and can't even run a normal country.<|endoftext|>\n",
            "<|startoftext|>Wow Donald Trump was far more effective and prepared than Crooked Hillary Clinton!<|endoftext|>\n",
            "<|startoftext|>The failing @nytimes has gotten better and more professional with time!<|endoftext|>\n",
            "<|startoftext|>Looking forward to getting this terrible and dangerous rigged system going - which will lead to big trouble for Dems!<|endoftext|>\n",
            "<|startoftext|>Hillary's terrible judgment calls into question our national security and our trade relationships<|endoftext|>\n",
            "<|startoftext|>Hillary Clinton would be unable to lead this country and should not be allowed to get her husband out of jail!<|endoftext|>\n",
            "<|startoftext|>My great honor to be honored &amp; presented \"Celebrate the Life of Bill Clinton\" at the @FLOTUSJBA @TrumpTowerNYC!… URL<|endoftext|>\n",
            "<|startoftext|>I will be on Fox &amp; Friends tomorrow morning at 7. Will be discussing the huge crowds in Nevada &amp; all over the country!<|endoftext|>\n",
            "<|startoftext|>Wow! Crowd very important to my win in New Hampshire. So amazing! Big crowds of enthusiastic people!<|endoftext|>\n",
            "<|startoftext|>Wow my biggest crowd yet recorded a video message to the parents of the child who did terrible things in Orlando. Will the parents be happy?<|endoftext|>\n",
            "<|startoftext|>Great poll in New Hampshire #NewSUNS Poll has me beating Hillary by 5 to 1. I will win the election very easily!<|endoftext|>\n",
            "<|startoftext|>I will be interviewed on @foxandfriends tomorrow morning at 7. Then I will be at my great golf course in Atlantic Canada to celebrate the results of the polls!<|endoftext|>\n",
            "<|startof\n",
            "\n",
            "[1300 | 4328.37] loss=1.35 avg=1.77\n",
            "[1400 | 4641.97] loss=1.52 avg=1.75\n",
            "======== SAMPLE 1 ========\n",
            "Entant will be named by the Senate Judiciary Committee into the Mueller Report which is on a far more than circumstantial basis than the Dems were before now even more CONFIRMED by the Jeff Sessions who can see through my lies and lies. They cannot allow Mueller to have independent charge. Must impeach Jeff Sessions!<|endoftext|>\n",
            "<|startoftext|>....however if there was NO COLLUSION there would not be an issue that would be discussed on our very bad &amp; highly conflicted Jeff Sessions. I have long since left any and all other Issues to his fine selection. He will be a Proclamation Managers' Proclamation Managers!<|endoftext|>\n",
            "<|startoftext|>“We take the fight to the enemy as easily demonstrated today. The Fake News Media is the enemy of the people.” Senator John Barrasso<|endoftext|>\n",
            "<|startoftext|>...of all the Fake News Media who have worked for and with me during the ridiculous Mueller Witch Hunt including the Fake Commentary from @TuckerCarlson who has worked for me (I think) very little Fake News. No matter what they write it is Fake it is Fake and we take the fight to the enemy as easily demonstrated today. The Fake News Media is the enemy of the people...<|endoftext|>\n",
            "<|startoftext|>“The people are the most intelligent and the most loyal that this Great Nation has ever had. They are the most loyal to this Great Nation that the Fake News Media has never been known to be!” Senator John Barrasso<|endoftext|>\n",
            "<|startoftext|>When the people of Scotland cast their vote for Brexit the will of the people was just as you would expect them to back the United Kingdom in a very big way - and in a very strong way!<|endoftext|>\n",
            "<|startoftext|>It was my great honor to welcome Prime Minister @AlexSalmond to the @WhiteHouse today with @FirstMinisters Meeting in the East Room of the Oval Room! URL<|endoftext|>\n",
            "<|startoftext|>Congratulations to the United Kingdom on what will be a Happy Birthday to both our great teams! URL<|endoftext|>\n",
            "<|startoftext|>It was a great pleasure to host the 24th Annual @MissUniverse Pageant today in the East Room of the Oval Room! URL<|endoftext|>\n",
            "<|startoftext|>“We take the fight to the enemy as easily demonstrated today. The Fake News Media is the enemy of the people. The Fake News Media is the Enemy of the People!” Chuck Hagel URL<|endoftext|>\n",
            "<|startoftext|>....there is no case for anyone except as an attorney general or a political hack for a criminal indictment but I do see the possibility it could be an appropriate action for him to take for all the reasons stated in the letter which you will read shortly. But as you know I wouldn’t want to see a case in such a situation because the Mueller investigation can’t go there in a criminal case!<|endoftext|>\n",
            "<|startoftext|>“We are going to take the fight to the enemy through all of the legal means necessary - but above all through his attorney general.” Henry Kissinger - National Defense University Address in 1973  URL<|endoftext|>\n",
            "<|startoftext|>The Fake News Media is working overtime to blame Republicans Congress and the conservative movement for the divisions and divisions that are tearing society apart. There are many causes for the problems within the Republican Party but perhaps my greatest of them is the unending fight against the Trump Agenda - the Anti-Trump agenda which is at hand. I have great respect for and support Luther Strange in the Alabama Primary. Look forward to working together for the good of our Great Country!<|endoftext|>\n",
            "<|startoftext|>....the Mueller team. This would mean that during the first two years of my Presidency there was NO COLLUSION which means I have already COMPLETE and OBSTRUCTIVE discussions with Jared (Sessions) and the Fake News Media of which there is not only the Fake TV and Fake Book writers. Jared has explained under oath that he only told me about these discussions when he was made a scapegoat to deflect responsibility. He has since stated under oath that they never happened.” Charles Payne - @Newsmax_Media<|endoftext|>\n",
            "<|startoftext|>The Witch Hunt Team continues to be one of Big Business! URL<|endoftext|>\n",
            "<|startoftext|>Today it was my great honor to welcome President Abdel Fatah al-Sisi of Egypt\n",
            "\n",
            "[1500 | 4993.22] loss=1.43 avg=1.72\n",
            "[1600 | 5307.08] loss=1.92 avg=1.74\n",
            "======== SAMPLE 1 ========\n",
            " has \"not been a perfect season\" -- and a top 5 defense!<|endoftext|>\n",
            "<|startoftext|>I never quit--even when it hurts. No matter what happens there will be a comeback.<|endoftext|>\n",
            "<|startoftext|>China is buying Boeing's (BA) new 787 Dreamliner in large orders. Great news for the U.S. I always believed we could do much better. In the meantime - bigger and better!<|endoftext|>\n",
            "<|startoftext|>I had my lawyers check with the FAA about my airplane getting a \"green\" on the \"blacklist\" because I am not on that list. In fact it just got worse!<|endoftext|>\n",
            "<|startoftext|>The new IAAF World Athletics Championships schedule has just happened--great going @usafa#TeamUSA  I always enjoyed being there.<|endoftext|>\n",
            "<|startoftext|>The World Cup will not be held in the Black Hole of space--it will be held in L.A.<|endoftext|>\n",
            "<|startoftext|>The U.S. needs to immediately stop the flights or be forced to fold as a country. Seriously!<|endoftext|>\n",
            "<|startoftext|>Remember to go out and buy a house—even if you don't have to. That's a good idea for all—and by the way you do not want to buy a house—you want to sell it.<|endoftext|>\n",
            "<|startoftext|>If I told you that the 2016 Election was rigged in favor of Donald Trump you would have voted for Crooked Hillary.<|endoftext|>\n",
            "<|startoftext|>Just had a great meeting with The Donald where we discussed the ongoing investigation into Paul Manafort.<|endoftext|>\n",
            "<|startoftext|>The 2016 Election was rigged in favor of Donald Trump. It was a Hillary Clinton victory that was so obviously done!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary Clinton tried the same thing in the Election. Lost. Too bad!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary tried her worst plan yet—ending marriage and starting new deal with Bernie. That was my great advice - do what I tell you to do. Crazy Bernie!<|endoftext|>\n",
            "<|startoftext|>The 2016 Election was rigged...Crooked Hillary lost easily and without a fight. Bernie ran a great race and got NO help!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary Clinton ran a great race and the Green Party picked 60000 more votes than Bernie in the Electoral College. In other words Crooked won by far!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary Clinton had a \"surprise moment\" in Ohio when voters went to prison. Bernie Sanders didn't even show up. So nice!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary Clinton got more votes in Electoral College than Bernie Sanders. So many votes...but it was the \"Super Delegates\"!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary Clinton said she doesn't talk about the rigged system because it's just not political. Super Electoral!<|endoftext|>\n",
            "<|startoftext|>I will be interviewed by @SeanHannity for tonight's @GOPDebate. Enjoy!<|endoftext|>\n",
            "<|startoftext|>Just won by a large majority in Alabama - big news!<|endoftext|>\n",
            "<|startoftext|>Big vote on the Supreme Court in Mississippi. Will go with the U.S. Constitution over the Electoral College in 2020!<|endoftext|>\n",
            "<|startoftext|>The Green Party is being ripped off by the Green Party so big that their candidate will never gain an Electoral College vote. They are doing so but a new candidate will be voted for!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary tried again for President. This time her campaign spent MUCH LESS. The race was close and she lost!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary Clinton spent more than half of her lifetime building up her bank account - time for her to reform the system!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary spent a fortune of her money on the debates while Bernie sat alone earning HIS. Why so unfair!<|endoftext|>\n",
            "<|startoftext|>Crooked Hillary\n",
            "\n",
            "[1700 | 5654.23] loss=1.72 avg=1.74\n",
            "[1800 | 5961.87] loss=1.28 avg=1.71\n",
            "======== SAMPLE 1 ========\n",
            " the time I first started writing it I didn’t believe it was accurate - and I changed it - I never knew how reliable that would be. But trust me on #FakeNews!<|endoftext|>\n",
            "<|startoftext|>Via @WSJ: “President Donald Trump signs bill strengthening family separation” URL<|endoftext|>\n",
            "<|startoftext|>The only thing that will save Social Security is the replacement of Ocare with a much better plan!<|endoftext|>\n",
            "<|startoftext|>The only way to replace Ocare is to take the money left over from the $5 Trillion cut &amp; replace it with a much better program.<|endoftext|>\n",
            "<|startoftext|>Republicans are holding Obama’s card on Social Security - but won’t vote for its replacement.<|endoftext|>\n",
            "<|startoftext|>Obama’s Social Security cuts do nothing for individuals - only apply to our current Social Security System.<|endoftext|>\n",
            "<|startoftext|>Great article today at @WashTimes’ @JeffJlpa1 @BretBaier<|endoftext|>\n",
            "<|startoftext|>@WashTimes @StephenBrathwaite Thanks.<|endoftext|>\n",
            "<|startoftext|>Why would Republicans be in favor of Ocare with its $5 Trillion annual deficits? Not even close.<|endoftext|>\n",
            "<|startoftext|>@WashTimes  I am not.<|endoftext|>\n",
            "<|startoftext|>@WashTimes  I think they are both correct.<|endoftext|>\n",
            "<|startoftext|>@WashTimes  I think the Republicans are right!<|endoftext|>\n",
            "<|startoftext|>@WashTimes  Thanks.<|endoftext|>\n",
            "<|startoftext|>It's the same old story -- we got what we wanted in Benghazi and now Congress shouldn't waste time on another investigation.<|endoftext|>\n",
            "<|startoftext|>Via @Newsmax_Media: “Donald Trump in Talks to Go on Tour with Ben Carson” URL<|endoftext|>\n",
            "<|startoftext|>If the Republicans are going to save Social Security then they should give Obama a pay raise. Spend more time on the economy then on the battle over Social Security.<|endoftext|>\n",
            "<|startoftext|>@WashTimes  Thanks<|endoftext|>\n",
            "<|startoftext|>Via @Newsmax_Media: “Donald Trump talks about going on tour”  URL<|endoftext|>\n",
            "<|startoftext|>The Republicans are wasting their time. Go back and study the Democrats policies in detail. Let them tell you how bad they are.<|endoftext|>\n",
            "<|startoftext|>Obama wants to cut Social Security Medicare Medicare and Medicaid–he must go--- this is not the \"End Game\" for the Old Media!<|endoftext|>\n",
            "<|startoftext|>The Republicans are wasting their time here--- take the issue off the table and move on to something else (namely impeach Obama). Democrats will cry shame<|endoftext|>\n",
            "<|startoftext|>I hope the people of the United States wake up and see that there is a big difference between the U.S. Republican Party and the Democrats Party!<|endoftext|>\n",
            "<|startoftext|>Congratulations to Tom Brady on his 12th Super Bowl win! Tom is a special player and special person.<|endoftext|>\n",
            "<|startoftext|>Congratulations to Tom Brady on @Patriots' Super Bowl win! Tom is a special player and special person.<|endoftext|>\n",
            "<|startoftext|>We have a total political scandal with the phony and fraudulent Clinton/Lynch investigation. A phony it was never going to be accepted as a fact! We need to start an actual Witch Hunt.<|endoftext|>\n",
            "<|startoftext|>Hillary Clinton is under investigation for all of the crimes she committed and all of the crimes she has admitted to in that she talked about running the Benghazi Email server.<|endoftext|>\n",
            "<|startoftext|>The Fake News media is trying as hard as they can to show the public that the election results are actually for Republicans. So when Trump wins a lot of votes the Fake News goes nuts. Sorry!<|endoftext|>\n",
            "<|startoftext|>Congratulations to Marco Rubio on his 20th consecutive\n",
            "\n",
            "[1900 | 6306.40] loss=1.41 avg=1.69\n",
            "[2000 | 6613.95] loss=1.21 avg=1.66\n",
            "Saving checkpoint/trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt/model-2000\n",
            "======== SAMPLE 1 ========\n",
            "> - Will be doing @gretawire on @fox and @oreillyfactor shortly - 2:30 A.M.\n",
            "<|startoftext|>I hope everybody goes to Macy's today to buy the awful shirt ties and apparel that Bret and @OMAROSA are cutting. Bad timing!<|endoftext|>\n",
            "<|startoftext|>@hayesondunham Thanks Michelle!<|endoftext|>\n",
            "<|startoftext|>My interview on @oreillyfactor discussing Mar-a-Lago and Trump National Doral: URL<|endoftext|>\n",
            "<|startoftext|>Trump Int'l Hotel and Tower Baltimore looks like a house you wouldn't mind buying - save your money - and renovate it ASAP!<|endoftext|>\n",
            "<|startoftext|>@TheHuskyRink @hollywebb You can't beat me on your own!<|endoftext|>\n",
            "<|startoftext|>@Tanya_Leigh  The Apprentice is dying-in the ratings it’s over! No audience!<|endoftext|>\n",
            "<|startoftext|>@OwenKelly @TrumpLasVegas @LilJon @ApprenticeNBC  I agree on both!<|endoftext|>\n",
            "<|startoftext|>@BeaumontAnthony @realDonaldTrump  Thanks Anthony!<|endoftext|>\n",
            "<|startoftext|>@Vincent_io @realDonaldTrump You have no hope of winning-you’ve proved it time and time again.<|endoftext|>\n",
            "<|startoftext|>@SJG24 I agree!<|endoftext|>\n",
            "<|startoftext|>@huffingtonpost  I hope so!<|endoftext|>\n",
            "<|startoftext|>@KrystianRuck  Thanks!<|endoftext|>\n",
            "<|startoftext|>\"When you have a dream it's a very powerful one.\"-- Albert Einstein<|endoftext|>\n",
            "<|startoftext|>@harry_brown @ApprenticeNBC @oreillyfactor  And yet it's sad that @HBO is going in that direction—(cont) URL<|endoftext|>\n",
            "<|startoftext|>@jeffthewitte It could be worth it—be sure.<|endoftext|>\n",
            "<|startoftext|>@SJG24 @IvankaTrump Thanks.<|endoftext|>\n",
            "<|startoftext|>@jimrobbins73 @TrumpDoral @billmaher Thanks Josh.<|endoftext|>\n",
            "<|startoftext|>@TheTodaysGolfer @realDonaldTrump If I were a buyer I'd buy a @billmaher shirt- thanks.<|endoftext|>\n",
            "<|startoftext|>@JNorr11 Great idea!<|endoftext|>\n",
            "<|startoftext|>@brentleystephens  I agree!<|endoftext|>\n",
            "<|startoftext|>@jessemonfortnive  I wonder if Iv is making enough money to buy a home-thanks!<|endoftext|>\n",
            "<|startoftext|>@LilB_Joy  What a great time you�re playing at the Trump @DoralResort-thanks.<|endoftext|>\n",
            "<|startoftext|>@LilB_Joy Thank you.<|endoftext|>\n",
            "<|startoftext|>@Lilsammy  Thanks.<|endoftext|>\n",
            "<|startoftext|>The great @IvankaTrump has been doing a fantastic job during my Presidency.<|endoftext|>\n",
            "<|startoftext|>Our great new Ambassador to the United Nations Nikki Haley has a big job on her hands so I expect she will do a great job. #UNGAURL<|endoftext|>\n",
            "<|startoftext|>President Nixon was the first President to call me and ask what the hell. I said OK and he was never pressured to do anything. Remember the Vietnam War?<|endoftext|>\n",
            "<|startoftext|>The biggest story of all-time is the Watergate scandal but the Dems have not even called how low the crime was. They still can't answer why no indictment!<|endoftext|>\n",
            "<|startoftext|>What is the excuse the Dems will not release a copy of the Servers or to their boss James Comey why are you refusing to give documents?<|endoftext|>\n",
            "<|startoftext|>\n",
            "\n",
            "[2100 | 6969.69] loss=0.59 avg=1.61\n",
            "[2200 | 7280.25] loss=1.50 avg=1.60\n",
            "======== SAMPLE 1 ========\n",
            " survivors - thank you.<|endoftext|>\n",
            "<|startoftext|>Wow I see what the Democrats are doing - they have put $10000000 on the table for the Democrat Celebration of CRIMINALS!!! CRIMINALS INCLUDING LOSER ARMS!<|endoftext|>\n",
            "<|startoftext|>The Democrat National Committee had a meeting that did not meet security requirements. Meeting was secretly held without any information...<|endoftext|>\n",
            "<|startoftext|>...and with no agenda. Another DNC lie that we will soon learn is the DNC Russia hoax.<|endoftext|>\n",
            "<|startoftext|>Thank you to Republican Senator Luther Strange who has my full endorsement and who is doing really well in the Alabama primary. Luther is strong on crime.<|endoftext|>\n",
            "<|startoftext|>URL<|endoftext|>\n",
            "<|startoftext|>Thank you Alabama - this is a MOVEMENT! URL<|endoftext|>\n",
            "<|startoftext|>.@KarlRove of losing Karl Rove - he is a loser who lost the primaries and is now fighting to protect Crooked Hillary Clinton!<|endoftext|>\n",
            "<|startoftext|>.@foxandfriends in two minutes - enjoy!<|endoftext|>\n",
            "<|startoftext|>URL<|endoftext|>\n",
            "<|startoftext|>URL<|endoftext|>\n",
            "<|startoftext|>Why are the Democrats allowing hundreds of thousands of foreign donations while they raise billions of dollars in the election? JOKE!<|endoftext|>\n",
            "<|startoftext|>“President Donald J. Trump Approves Emergency Declarations”➡️URL @WhiteHouse URL<|endoftext|>\n",
            "<|startoftext|>“This week the President has declared a National Emergy to address the crisis of MS-13. ‘Weekly Address’ President Donald J. Trump Declarations the First Step to Fix the Unitary System” URL<|endoftext|>\n",
            "<|startoftext|>“President Donald J. Trump is Administrator in the Oval Office...” URL<|endoftext|>\n",
            "<|startoftext|>MAKE AMERICA GREAT AGAIN! URL<|endoftext|>\n",
            "<|startoftext|>I will be interviewed on @foxandfriends tomorrow morning at 9.00 A.M.<|endoftext|>\n",
            "<|startoftext|>Thank you Mississippi - on Monday I will be joining you at your Southern Border for a rally at the Nissan Stadium. Join me to #MAGATickets: URL URL<|endoftext|>\n",
            "<|startoftext|>MAKE AMERICA GREAT AGAIN! URL<|endoftext|>\n",
            "<|startoftext|>URL<|endoftext|>\n",
            "<|startoftext|>HAPPY BIRTH DAY! JOBS JOBS JOBS! #MAGA URL<|endoftext|>\n",
            "<|startoftext|>HAPPY BIRTH DAY TO ALL AMERICANS! URL<|endoftext|>\n",
            "<|startoftext|>Thank you Tennessee - on Monday I will be joining you for a road rally at the Nashville Civic Center. Join us on the steps of City Hall: URL<|endoftext|>\n",
            "<|startoftext|>Join me in Mississippi on Monday night at the Nissan Stadium - and together we will MAKE AMERICA GREAT AGAIN! Tickets: URL URL<|endoftext|>\n",
            "<|startoftext|>Congratulations to Karen Handel on her BIG victory in the special election for Georgia 6th. We are with you today!<|endoftext|>\n",
            "<|startoftext|>Thank you America - on Monday I will be joining you at 6:00pm - and together we will MAKE AMERICA GREAT AGAIN!<|endoftext|>\n",
            "<|startoftext|>Thank you South Carolina - join me at the Tennessee State Fairgrounds. On the steps of City Hall: URL URL<|endoftext|>\n",
            "<|startoftext|>I am in Tennessee - joining @GovMikeHuckabee &amp; @MarkHarrisNM for a news conference at 7:00pm!<|endoftext|>\n",
            "<|startoftext|>URL<|endoftext|>\n",
            "<|startoftext|>MAKE AMERICA GREAT AGAIN! URL<|endoftext|>\n",
            "<|startoftext|>URL<|endoftext|>\n",
            "<|startoftext|>We have to secure our borders - and you have every right\n",
            "\n",
            "[2300 | 7628.33] loss=1.15 avg=1.58\n",
            "[2400 | 7940.03] loss=0.59 avg=1.53\n",
            "======== SAMPLE 1 ========\n",
            " lead and the whole World was looking. I wasn’t and don’t know how that story got started.<|endoftext|>\n",
            "<|startoftext|>“Never confuse a single defeat with a final defeat.” - Thomas A. Edison<|endoftext|>\n",
            "<|startoftext|>There is a wonderful man in Washington D.C. named Scott Pruitt who is going to do a fantastic job. My endorsement is very much in his favor.<|endoftext|>\n",
            "<|startoftext|>My thoughts on Scott Pruitt to be EPA Administrator: URL<|endoftext|>\n",
            "<|startoftext|>\"A person never sets out to fail. Those who set out to succeed are the ones who stick in there and do the job.\"- Think Like a Champion<|endoftext|>\n",
            "<|startoftext|>We have a long way to go before we are truly tough and fair. But if we make the changes necessary he will be a terrific leader.<|endoftext|>\n",
            "<|startoftext|>If Democrats really want to achieve historic tax cuts and raise millions for their candidates &amp; causes then they should fight hard for these goals in 2018 and 2025. Budget control!<|endoftext|>\n",
            "<|startoftext|>In order to secure our Borders and to help STOP the massive inflow of Drugs &amp; Humanitarian Aid into the Southern Hemisphere from South America they must have the money to go to new and improved facilities. Congress must act fast!<|endoftext|>\n",
            "<|startoftext|>The Democrats that are in control right now in Congress must give us the money to finish our Wall and get the badly needed Border Wall!<|endoftext|>\n",
            "<|startoftext|>Congratulations to our great Miss Universe on winning the Miss Universe Pageant in Russia even though we had a tremendous turnout of more than 200 Million people. We will all be proud of her in Moscow on Nov 9 2018<|endoftext|>\n",
            "<|startoftext|>Congrats to new Miss Universe on her incredible win in Russia. She came in 2nd place by 14000 votes and won in a landslide. The judges were incredible with her being crowned Miss Universe. The Country is finally O.K.!<|endoftext|>\n",
            "<|startoftext|>Congratulations to Miss Virginia Hicks for winning the Miss Universe Pageant in Russia. We are going to miss you and of course our great Miss Universe contestants.<|endoftext|>\n",
            "<|startoftext|>Congratulations to Miss Georgia Hicks on winning the Miss Universe Pageant in Russia. U.S. is doing much better than it was with you and I to much improved and great country. Big day in Russia!<|endoftext|>\n",
            "<|startoftext|>So many problems in the U.S. and leadership that is hopeless and failing. Fiscal Cliff is not a deal breaker it is a very simple deal that will eventually get done. Be patient and never give up!<|endoftext|>\n",
            "<|startoftext|>In the U.S. a person has to commit a crime in order for us to get the Federal$ they gave us to fix it. We have become so dependent on foreign revenue that our borders are a dead stop. We have debt and we need to fix it now!<|endoftext|>\n",
            "<|startoftext|>Congratulations to Miss Virginia Hicks in winning the Miss Universe Pageant in Russia. We love u!<|endoftext|>\n",
            "<|startoftext|>When a person cheats on their President (and sometimes I disagree with them) it means both sides cheated on the election. It is now the President who is out of the question. The losers are supposed to be on the other side.<|endoftext|>\n",
            "<|startoftext|>My @gretawire interview where I said this year’s Miss Universe Pageant was close #2 on TV and \"the number one telecast\" URL<|endoftext|>\n",
            "<|startoftext|>Wow the @MissUniverse judges gave @realDonaldTrump a perfect score - and then they said something really bad about him. The judges were so negative he has to live with it!<|endoftext|>\n",
            "<|startoftext|>“Success is having to worry about every damn thing in the world except money.” -- Mark Krania<|endoftext|>\n",
            "<|startoftext|>Congratulations to @MissUniverse on its great ratings for the show. Congratulations to all of our contestants.<|endoftext|>\n",
            "<|startoftext|>Watch me on @FoxNews with @gretawire discussing why\n",
            "\n",
            "[2500 | 8290.06] loss=1.63 avg=1.54\n",
            "[2600 | 8602.28] loss=1.38 avg=1.53\n",
            "======== SAMPLE 1 ========\n",
            " them on Thursday night and Friday morning. #TBT<|endoftext|>\n",
            "<|startoftext|>Trump International Golf Club Scotland is situated on 300 pristine acres and fronts 2 golf courses. Spectac!URL<|endoftext|>\n",
            "<|startoftext|>\"The only sure way to get creative is to stay current with current events and trends.\" – Midas Touch<|endoftext|>\n",
            "<|startoftext|>With the economy still too poor to have a functioning market this will last for a very long time...<|endoftext|>\n",
            "<|startoftext|>... Even if they are not as healthy as they think they are.  #CelebApprentice   URL<|endoftext|>\n",
            "<|startoftext|>My sons Don Eric and Donal Eric will be on @greta on at 7:00. A long time in this business - the best! #CelebApprentice<|endoftext|>\n",
            "<|startoftext|>In The Center Of The Storm @TrumpTowerNY is #1 destination in the NYC region. @B&amp;D @TrumpSoHo are NYC's top hotel condos &amp; luxury restaurant URL<|endoftext|>\n",
            "<|startoftext|>I would like to wish everyone including myself a Happy Birthday and a Happy New Year. May your years of health and success be long and healthy ones.<|endoftext|>\n",
            "<|startoftext|>I look forward to New Year's Eve at the Plaza. Will be a great view and celebration venue! URL<|endoftext|>\n",
            "<|startoftext|>Be sure to turn January 15 into one that you will be proud to be part of. #CelebApprentice<|endoftext|>\n",
            "<|startoftext|>It was #CelebApprentice that sold most of its episodes on @YouTube for $4.4 billion URL Great deal for NBC!<|endoftext|>\n",
            "<|startoftext|>On Celebrity Apprentice with Larry David- #CelebApprentice<|endoftext|>\n",
            "<|startoftext|>Celebrity Apprentice with Larry David- #CelebApprentice<|endoftext|>\n",
            "<|startoftext|>Watch The Apprentice on Wednesday Jan. 15th.URL<|endoftext|>\n",
            "<|startoftext|>Watched @OMAROSA on @DatelineNBC tonight. URL<|endoftext|>\n",
            "<|startoftext|>Watch Apprentice this Sunday on NBC at 9PM EST. It's an exciting new season getting ready to rock!<|endoftext|>\n",
            "<|startoftext|>I wonder if Barack Obama will use the drone to harass some of the more innocent people in Boston?<|endoftext|>\n",
            "<|startoftext|>A-Rod's problem is with the NFL not the Yankees. I hope A-Rod is suspended for rest of season not fined.<|endoftext|>\n",
            "<|startoftext|>We're all a little sad that A-Rod's suspension will be in place while Roger is out. Roger is a great friend.<|endoftext|>\n",
            "<|startoftext|>Can't wait to watch the new Celebrity Apprentice this Sunday evening. URL<|endoftext|>\n",
            "<|startoftext|>A-Rod's problem is with the NFL not the Yankees. I hope A-Rod is suspended for rest of season not fined.<|endoftext|>\n",
            "<|startoftext|>The terrorist who killed people in Boston shouldn't have been in there yesterday <|endoftext|>\n",
            "<|startoftext|>Don't forget to watch Celebrity Apprentice this Sunday night at 9pm EST. It's an exciting new season getting ready to rock!<|endoftext|>\n",
            "<|startoftext|>Can't wait to watch the new Celebrity Apprentice this Sunday evening.  URL<|endoftext|>\n",
            "<|startoftext|>Celebrity Apprentice with Larry David tonight at 9pm EST.  Celebrity Apprentice  URL<|endoftext|>\n",
            "<|startoftext|>The terrorists who killed people in Boston shouldn't have been in there yesterday.<|endoftext|>\n",
            "<|startoftext|>In today's #trumpvlog I discuss  how badly I was mistreated by @Yankees Dexter Fowler and Derek Zernon... URL<|endoftext|>\n",
            "<|startoftext|>Why am I doing an interview with Bill O'Reilly on Fox this morning? Bill is a total waste of airtime. @FoxBusiness<|endoftext|>\n",
            "<|startoftext\n",
            "\n",
            "[2700 | 8951.83] loss=1.38 avg=1.53\n",
            "[2800 | 9263.80] loss=1.61 avg=1.53\n",
            "======== SAMPLE 1 ========\n",
            "oftext|>\n",
            "<|startoftext|>#TRUMP T-Shirts available @ menswearpremier @ Macy's URL<|endoftext|>\n",
            "<|startoftext|>An attack is being planned on our country by radical Islam. They plan to kill innocent people and maybe even break our laws. Be careful we need to get smart fast!<|endoftext|>\n",
            "<|startoftext|>I am in Iowa getting ready to speak. People are shooting at both entrances getting ready for a major riot! Big traffic cones and armored vehicles. Be safe!<|endoftext|>\n",
            "<|startoftext|>Stock Market hit another all-time high yesterday on an unrelated subject!<|endoftext|>\n",
            "<|startoftext|>Stock Market up almost 50% since the Election with 7.2 trillion dollars of U.S. value built by American workers! Unemployment down to 4.8%, lowest ever in jobs!<|endoftext|>\n",
            "<|startoftext|>Great job @foxandfriends with their very unbiased reporting. Always seeking the truth (and I mean it). Sometimes you have to fight your opponents in a political debate. Thanks!<|endoftext|>\n",
            "<|startoftext|>Great job @foxandfriends with their very unbiased reporting. Always seeking the truth (and I mean it). Sometimes you have to fight your opponents in a political debate. Thanks!<|endoftext|>\n",
            "<|startoftext|>MAKE AMERICA GREAT AGAIN! URL<|endoftext|>\n",
            "<|startoftext|>I will be interviewed on @foxandfriends at 9:30 - 10:00 A.M. (Eastern). ENJOY!<|endoftext|>\n",
            "<|startoftext|>Inconceivable that our very weak and conflicted Politicians would talk of lifting sanctions on Russia when they have NOT lifted a single sanction since the election!<|endoftext|>\n",
            "<|startoftext|>Just heard Foreign Minister Sergei Lavrov and his family visiting. I see Russia is not helping with North Korea - just talking. Too bad!<|endoftext|>\n",
            "<|startoftext|>Russia is not helping with North Korea. Perhaps we could use a little \"glue.\" I don't believe China is happy because they watched as U.S. lead from the beginning<|endoftext|>\n",
            "<|startoftext|>Today it was my true privilege to defend the American flag at the @NCAA Convocation in Nashville Tennessee. Thank you! URL<|endoftext|>\n",
            "<|startoftext|>The only people upset with me are the people who saw my vision for the American Dream - allowing ALL Americans to succeed. They have no vision and I have no patience for them!<|endoftext|>\n",
            "<|startoftext|>....is the most highly conflicted of the five (or six). Isn't that bad? Isn't that the most important (and yet seldom talked about) conflict of all? Be careful...Can you imagine if I'm right (and I sometimes am).....<|endoftext|>\n",
            "<|startoftext|>The biggest conflict of all is the so-called Russian Witch Hunt. Let’s see if this proves to be any different. Remember the Collusion Delusion...<|endoftext|>\n",
            "<|startoftext|>“You can’t build a wall from scratch. You need the skills you get growing. It’s very simple. You need people who know how to build.” Ben Carson at CPAC #MAGA<|endoftext|>\n",
            "<|startoftext|>The Democrats are in trouble for their very weak and conflicted \"Russia\" policy which is going to put lives at risk. At some point I’ll start looking at funding border security specifically...<|endoftext|>\n",
            "<|startoftext|>The Democrats have become the party of Shutdown. They want to shut down the government early from the September 18th date of funding the WALL. Bad “RADICAL.”<|endoftext|>\n",
            "<|startoftext|>So the long awaited Mueller Report conclusions are a far cry from the lies and leaks of the past. Some of the Holder Lynch and Brennan Brennan members....<|endoftext|>\n",
            "<|startoftext|>The Democrats are moving so fast to politicize the facts. They are taking all of my reports (especially the Comey Memos) and politicizing them at a level not seen before.” Ben Carson @RealBenCarson<|endoftext|>\n",
            "<|startoftext|>....high crimes and misdemeanors. In addition to failing\n",
            "\n",
            "interrupted\n",
            "Saving checkpoint/trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt/model-2819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, sample_every, sample_length, sample_num, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, overwrite)\u001b[0m\n\u001b[1;32m    303\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mopt_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                     feed_dict={context: sample_batch()})\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5d9ac2214ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m               \u001b[0msample_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m               \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m               )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(sess, dataset, steps, model_name, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, sample_every, sample_length, sample_num, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, overwrite)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'interrupted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36msave\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             global_step=counter-1)\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1171\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1172\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hIfY52cMbad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYMJPlF_McPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ade0RkR0EIi3",
        "colab_type": "code",
        "outputId": "e39b1811-8f4f-4f6e-8c72-17c52b8226de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmx5DKQd0LIo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7agiDIcNw2oY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgLy5mlH0Mr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "e3e90f58-ba27-42a8-d966-2a635a3d1fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0728 00:30:30.897094 140544924612480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:90: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0728 00:30:30.899383 140544924612480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:100: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0728 00:30:32.196897 140544924612480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:340: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0728 00:30:32.214372 140544924612480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0728 00:30:37.458852 140544924612480 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py:344: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0728 00:30:40.023486 140544924612480 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt/model-2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "text = gpt2.generate(sess, run_name='trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt',length=140,truncate='<|endoftext|>',include_prefix=False,prefix='<|startoftext|>', nsamples=2000, return_as_list=True, batch_size=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRoazpHVCjDS",
        "colab_type": "code",
        "outputId": "bc5b5f80-3143-48df-d3db-636c652df656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = pd.DataFrame({'col':text})\n",
        "df.head()\n",
        "df['class']='0'\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Just left the @WhiteHouse. Amazing journey of ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The @nyjets are getting good play from their d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@Natalie_Grund Fear not we will have a movie s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I hope people stop and think about what is goi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>“@ApprenticeNBC Don't be afraid of mistakes. U...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 col class\n",
              "0  Just left the @WhiteHouse. Amazing journey of ...     0\n",
              "1  The @nyjets are getting good play from their d...     0\n",
              "2  @Natalie_Grund Fear not we will have a movie s...     0\n",
              "3  I hope people stop and think about what is goi...     0\n",
              "4  “@ApprenticeNBC Don't be afraid of mistakes. U...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QQVfjQPEUVh",
        "colab_type": "code",
        "outputId": "50b0c3b9-4d60-4859-9c87-7acae2720f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "df2 = pd.read_fwf(\"/content/trump-tagged-no-retweets-06-14-2019-delimitersremovednolinks.txt\")\n",
        "df2['class']='1'\n",
        "df2.head()\n",
        "spam_ham_trump = df.append(df2)\n",
        "#spam_ham_trump.head()\n",
        "spam_ham_trump.describe()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>28318</td>\n",
              "      <td>28318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>28124</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>MAKE AMERICA GREAT AGAIN!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>27</td>\n",
              "      <td>26318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              col  class\n",
              "count                       28318  28318\n",
              "unique                      28124      2\n",
              "top     MAKE AMERICA GREAT AGAIN!      1\n",
              "freq                           27  26318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH-3UuXSBAxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfimport numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WEa1mNjI_SM",
        "colab_type": "code",
        "outputId": "ed829334-bbfa-49dc-b097-c1c0c9eccb82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip install spacy\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79igzo39I8EG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJBDfGsoLOvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return text.strip().lower()\n",
        "  \n",
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKTCjQXlLog7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = spam_ham_trump[\"col\"]\n",
        "ylabels = spam_ham_trump[\"class\"]\n",
        "\n",
        "#X = df_amazon['verified_reviews'] # the features we want to analyze\n",
        "#ylabels = df_amazon['feedback'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQjewzxNuot",
        "colab_type": "code",
        "outputId": "d080160f-6c6b-4a90-a688-e622e886a342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "#classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "#from sklearn.svm import SVC\n",
        "#clf = SVC(gamma='auto')\n",
        "clf.fit(X, y) \n",
        "pipe.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('cleaner', <__main__.predictors object at 0x7fd0dbdb18d0>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 t...\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7fd0774a78c8>,\n",
              "                                 vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='warn', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='warn', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlEMbf1LTYO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEoRhIrfOEBb",
        "colab_type": "code",
        "outputId": "8a9b3a64-4ffa-4475-838b-1183c1d5541b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "#print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "#print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.94515065913371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1243: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if pos_label not in present_labels:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-207359ec676b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Model Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Regression Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Regression Precision:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Regression Recall:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1567\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpresent_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m                     raise ValueError(\"pos_label=%r is not a valid label: \"\n\u001b[0;32m-> 1246\u001b[0;31m                                      \"%r\" % (pos_label, present_labels))\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: pos_label=1 is not a valid label: array(['0', '1'], dtype='<U1')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRyUmGmoKhX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "f = open('twillio-load-trump-tagged-no-retweets-06-14-2019-delimitersadded.txt', 'w')\n",
        "json.dump(text, f)\n",
        "f.close()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5AjaBEdOCn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "ec1fc622-c3fe-43e5-9312-f26cec6e52fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3679
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LORD FITZWATER:\n",
            "Saw'st thou not, sir?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Madam, didst thou not mean, as some do,\n",
            "To cram others with beggary?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Ha, ha! what should you say? why didst thou say no?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Hath this woman in this world hungrier than man?\n",
            "Think'st thou that all these woes can be avoided?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "As many lives she hath prevented as man.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Why, hark! the Tower is brought down.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The King of Naples comes apace.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "It shall be so, I hope.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The Tower comes down.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "The Naples comes apace.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "It shall be so, I hope.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "\n",
            "====================\n",
            "\n",
            "LORDY:\n",
            "Every man that's gone hath died\n",
            "And that's the end of all.\n",
            "\n",
            "GLOUCESTER:\n",
            "Your lordship must be revenged, Hermione.\n",
            "\n",
            "LADY ANNE:\n",
            "And she shall be the man that I dreamt of.\n",
            "\n",
            "GLOUCESTER:\n",
            "I cannot sleep: never a waking man shall\n",
            "Go to any sleepder than to sleep.\n",
            "\n",
            "LADY ANNE:\n",
            "It shall be such a dream, I'll bear witness to it,\n",
            "That I may believe what I say.\n",
            "\n",
            "GLOUCESTER:\n",
            "All tongues to hell: never man to man,\n",
            "Let light hear me?\n",
            "\n",
            "LADY ANNE:\n",
            "No, I'll not bear witness to it.\n",
            "\n",
            "GLOUCESTER:\n",
            "I said you had heard it, but could not put\n",
            "You in words.\n",
            "\n",
            "LADY ANNE:\n",
            "It is a strange tale, the tale doth it:\n",
            "Hermione's great care is felling on wild Bolingbroke,\n",
            "And Bolingbroke is the king.\n",
            "\n",
            "GLOUCESTER:\n",
            "I could not put mine hand\n",
            "====================\n",
            "\n",
            "LORD FITZWATER:\n",
            "On pain of death, will he live to see this day\n",
            "His country requite itself again.\n",
            "\n",
            "DUKE OF YORK:\n",
            "What country?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Nor none but himself,\n",
            "That is, most certainly, from this day on\n",
            "My proudest unknown man shall be your king.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Of all men I am the one who shall lose.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Take thou the crown, and crown it well\n",
            "With all the proudest that have touch'd it.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I'll have the crown too, and set it on me.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Now York shall stand for his wish.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Stand you like a leader, like a great king\n",
            "And will you yield all obedience to him?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "I'll be king, I vow thee.\n",
            "\n",
            "DUKE OF YORK:\n",
            "And shall stand for that aim so well\n",
            "In all\n",
            "====================\n",
            "\n",
            "LORD FITZWATER:\n",
            "Sir, your company is witness to that.\n",
            "\n",
            "Servant:\n",
            "The duke shall have a chamber-maid,\n",
            "And not an officer of the people, to take\n",
            "The absolute duke of his city.\n",
            "\n",
            "GLOUCESTER:\n",
            "That's as many rogues as there are.\n",
            "\n",
            "Servant:\n",
            "Nine!\n",
            "\n",
            "Servant:\n",
            "Nine rogues!\n",
            "\n",
            "Servant:\n",
            "Nine!\n",
            "\n",
            "GLOUCESTER:\n",
            "A foul witch! Undone, go with her:\n",
            "This jade shall be queen of the dead.\n",
            "\n",
            "Servant:\n",
            "I'll not be queen.\n",
            "\n",
            "Servant:\n",
            "By any other name\n",
            "Will destroy Richard's crown,\n",
            "And London enter in a new sense.\n",
            "\n",
            "Lord:\n",
            "What is the king's name?\n",
            "\n",
            "Lord:\n",
            "Richard's name; 'tis the name of the duke slain\n",
            "In your famous battle.\n",
            "\n",
            "Servant:\n",
            "Nine!\n",
            "\n",
            "Lord:\n",
            "Nine!\n",
            "\n",
            "Servant:\n",
            "Nine!\n",
            "\n",
            "GLOUCESTER:\n",
            "The king!\n",
            "\n",
            "Servant:\n",
            "Nine!\n",
            "\n",
            "Lord:\n",
            "\n",
            "====================\n",
            "\n",
            "LORD STANLEY:\n",
            "Away with her!\n",
            "\n",
            "CATESBY:\n",
            "What counsel pluck, you, or me, or any of your party?\n",
            "\n",
            "WARWICK:\n",
            "Fear me not: I'll tell her she hath made a wrong.\n",
            "\n",
            "CATESBY:\n",
            "I shall feel it, sir, when I see it.\n",
            "\n",
            "WARWICK:\n",
            "Now, how shall we avoid her?\n",
            "\n",
            "KING HENRY VI:\n",
            "By living by, I'll hate myself and my tongue,\n",
            "And never bid farewell my body.\n",
            "\n",
            "CATESBY:\n",
            "'Tis a disgrace to live by,\n",
            "And I will hate myself as well as any\n",
            "Whom fortune hath held dear dear.\n",
            "\n",
            "WARWICK:\n",
            "What is the matter, my lord?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Get you hence, gentle queen, by way of reprieve:\n",
            "The Earl of Hereford, with all the rest of Spain,\n",
            "Staying in London till the war be done,\n",
            "And Goverrus with the rest of England,\n",
            "To prevent this loss of life.\n",
            "\n",
            "CATESBY:\n",
            "This is the business.\n",
            "\n",
            "NORTHUM\n",
            "====================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp and then download it.\n",
        "\n",
        "You can rerun the cell as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyqcJF_4Z0DX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ",length=140,truncate='<|endoftext|>',include_prefix=False,prefix='<|startoftext|>', nsamples=2000, return_as_list=True, batch_size=20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI7kRPhccIEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "outputId": "5f7866c2-0d1e-4a9e-9d29-46a00242038e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=140,\n",
        "                      temperature=0.9,\n",
        "                      nsamples=2000,\n",
        "                      batch_size=20,\n",
        "                      truncate='<|endoftext|>',\n",
        "                      include_prefix=False,\n",
        "                      prefix='<|startoftext|>',\n",
        "                      run_name='trump-tagged-no-retweets-06-14-2019-delimitersadded-symbol-for-url2.txt',\n",
        "                      top_k=40\n",
        "                      )\n",
        "\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0713 14:54:25.650161 140501625440128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:71: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0713 14:54:25.677207 140501625440128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0713 14:54:25.682593 140501625440128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail or out-of-memory/OOM), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot64slZptaXO",
        "colab_type": "code",
        "outputId": "605646cc-c37e-4e9f-e736-07788c9c958d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "import pandas\n",
        "df = pandas.read_csv(\"/content/subset.csv\",header=0)\n",
        "df['text']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>text</th>\n",
              "      <th>created_at</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>is_retweet</th>\n",
              "      <th>id_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>Today’s shooting in El Paso Texas was not only...</td>\n",
              "      <td>08-04-2019 04:19:01</td>\n",
              "      <td>25799</td>\n",
              "      <td>146075</td>\n",
              "      <td>False</td>\n",
              "      <td>1157868518823596032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>....Melania and I send our heartfelt thoughts ...</td>\n",
              "      <td>08-04-2019 04:19:01</td>\n",
              "      <td>15893</td>\n",
              "      <td>108001</td>\n",
              "      <td>False</td>\n",
              "      <td>1157868519964499968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>The people on the stage tonight and last were ...</td>\n",
              "      <td>08-01-2019 04:05:15</td>\n",
              "      <td>28886</td>\n",
              "      <td>144147</td>\n",
              "      <td>False</td>\n",
              "      <td>1156777889100173313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               source  ...               id_str\n",
              "0  Twitter for iPhone  ...  1157868518823596032\n",
              "1  Twitter for iPhone  ...  1157868519964499968\n",
              "2  Twitter for iPhone  ...  1156777889100173313\n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXbEDNG4uuKn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RetLNnY2uu_s",
        "colab_type": "code",
        "outputId": "2cb28618-3ed9-4863-ec1a-7e5d7060e54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "df['text']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Today’s shooting in El Paso Texas was not only...\n",
              "1    ....Melania and I send our heartfelt thoughts ...\n",
              "2    The people on the stage tonight and last were ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}