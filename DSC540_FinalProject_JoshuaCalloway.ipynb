{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/joshuacalloway/dsc540groupproject/blob/main/StartingTrumpTweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Natural Language Processing on Trump's Tweets\n",
    "- Joshua Calloway\n",
    "- DSC 540, Fall Quarter - DePaul\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Project\n",
    "We are looking Trump's tweets and applying NLP to see if we can build a classifier to predict sentiment on new tweets.\n",
    "Trump tweets are widely available, and we can run the project on either small set of 1000 tweets or larger set of 55,000 tweets.  All of these tweets are available on thetrumparchive and @realDonaldTrump twitter handle\n",
    "\n",
    "I selected the [Azure Predictive Analysis](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/) to create the **ground truth** sentiment labels.  Azure produces labels of **neutral, positive, negative**.\n",
    "\n",
    "with the tweets, we then split it into training and validation ( 90 percent of data ) and test data ( 10 percent of data ). With the training data, we apply the various methods\n",
    "\n",
    "- [Tweepy](https://www.tweepy.org/) and [TextBlob](https://textblob.readthedocs.io/en/dev/) for analyzing sentiment\n",
    "- [LogisticRegression](https://www.twilio.com/blog/2017/12/sentiment-analysis-scikit-learn.html) using the Tweepy/TextBlob to train the model\n",
    "- NLTK toolkit and NaiveBayesClassifier using the Tweepy/TextBlob to train the model\n",
    "- [FastText]((https://fasttext.cc) with single label sentiment using the Tweepy/TextBlob to train the model\n",
    "    - FastText with hyper-parameter tuning of epochs, learning rate, ngrams using the [fasttext autotune](https://fasttext.cc/docs/en/autotune.html) against the validation file\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data, Trump Tweets ( either 1000 or 55,0000 tweets )\n",
    "Here we use thetrumparchive to either fetch 1000 or larger set of 55,000 tweets.  The tweets come back as JSON in format of\n",
    "<code>\n",
    "{\n",
    "  id: 1\n",
    "  text: 'Lets win Michigan'\n",
    "  isRetweet: True\n",
    "  isDeleted: False\n",
    "  device: iPhone\n",
    "  favorites: 323,\n",
    "  retweets: 2\n",
    "  date: 2020-11-02\n",
    "}\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BikM0IG41OX0"
   },
   "outputs": [],
   "source": [
    "import urllib.request, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrZYjcyF1b5l",
    "outputId": "5869b244-d003-4a79-f451-1b3f26c9377c"
   },
   "outputs": [],
   "source": [
    "# If LargeData is True, then we fetch 55,0000 tweets\n",
    "def fetch_data(largeData=False):\n",
    "    if largeData:\n",
    "        with open('tweets_11-06-2020.json') as f:\n",
    "            data = json.load(f)  # fetch 50,0000 tweets\n",
    "    else:\n",
    "        with urllib.request.urlopen(\"https://www.thetrumparchive.com/latest-tweets\") as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "    return DataFrame(data)\n",
    "\n",
    "# get_tweet_text = lambda tweet : tweet['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can either fetch 1000 or 55,000 tweets by switching the flag largeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "4U_YshEn1kFL",
    "outputId": "39a62dc0-c2e1-4b18-be0b-d1ded182e33e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>isDeleted</th>\n",
       "      <th>device</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1329963571250335744</td>\n",
       "      <td>https://t.co/YHscjY6G8t</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>105034</td>\n",
       "      <td>29537</td>\n",
       "      <td>2020-11-21T01:43:19.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1329963296854847492</td>\n",
       "      <td>https://t.co/OLZnCJq93Y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>78420</td>\n",
       "      <td>20742</td>\n",
       "      <td>2020-11-21T01:42:14.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1329963239170564098</td>\n",
       "      <td>https://t.co/cwOQLhQNFq</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>126359</td>\n",
       "      <td>32853</td>\n",
       "      <td>2020-11-21T01:42:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1329871920607744001</td>\n",
       "      <td>RT @WhiteHouse: LIVE: President @realDonaldTru...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>18564</td>\n",
       "      <td>2020-11-20T19:39:08.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1329871776889925636</td>\n",
       "      <td>...Why won’t they do it, and why are they so f...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>152441</td>\n",
       "      <td>25486</td>\n",
       "      <td>2020-11-20T19:38:34.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1329963571250335744                            https://t.co/YHscjY6G8t   \n",
       "1  1329963296854847492                            https://t.co/OLZnCJq93Y   \n",
       "2  1329963239170564098                            https://t.co/cwOQLhQNFq   \n",
       "3  1329871920607744001  RT @WhiteHouse: LIVE: President @realDonaldTru...   \n",
       "4  1329871776889925636  ...Why won’t they do it, and why are they so f...   \n",
       "\n",
       "   isRetweet  isDeleted              device favorites retweets  \\\n",
       "0      False      False  Twitter for iPhone    105034    29537   \n",
       "1      False      False  Twitter for iPhone     78420    20742   \n",
       "2      False      False  Twitter for iPhone    126359    32853   \n",
       "3       True      False  Twitter for iPhone         0    18564   \n",
       "4      False      False  Twitter for iPhone    152441    25486   \n",
       "\n",
       "                       date  \n",
       "0  2020-11-21T01:43:19.000Z  \n",
       "1  2020-11-21T01:42:14.000Z  \n",
       "2  2020-11-21T01:42:00.000Z  \n",
       "3  2020-11-20T19:39:08.000Z  \n",
       "4  2020-11-20T19:38:34.000Z  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we r interested in the text for NLP\n",
    "data = fetch_data(largeData=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We split the data into training, validation and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a Y with unknown value\n",
    "data['sentiment'] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['text']\n",
    "y = data['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=555)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.30, random_state=555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use Azure Predictive Analysis to create the ground truth sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "credential = AzureKeyCredential(\"cb61d607e5c8402b9742b8aa40207593\")\n",
    "text_analytics_client = TextAnalyticsClient(endpoint=\"https://trumptweetanalysissentiment.cognitiveservices.azure.com/\", credential=credential)\n",
    "    \n",
    "def call_azure(list_text_only_ten_items):\n",
    "    response = text_analytics_client.analyze_sentiment(list_text_only_ten_items)\n",
    "    successful_responses = [doc for doc in response if not doc.is_error]\n",
    "    return list_text_only_ten_items, list(map(lambda x: x['sentiment'], successful_responses))\n",
    "    \n",
    "# treat mixed and neutral sentiment as neutral\n",
    "def combine_mixed_neutral(sentiments):\n",
    "    converted = []\n",
    "    for item in sentiments:\n",
    "        newitem = 'neutral' if item == 'mixed' else item\n",
    "        converted.append(newitem)\n",
    "    return converted\n",
    "           \n",
    "# this is ground truth.  Using Azure sentiment\n",
    "def calculate_groundtruth_sentiment(list_of_texts):\n",
    "    sublists = np.split(np.array(list_of_texts.tolist()), list_of_texts.size / 10)\n",
    "    retvalues = list(map(lambda ls: call_azure(list(ls)), sublists))\n",
    "    sentiments = []\n",
    "    for item in retvalues:\n",
    "        sentiments.append(item[1])    \n",
    "    sentiments = [item for items in sentiments for item in items]\n",
    "    return list_of_texts, combine_mixed_neutral(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test_groundtruth = calculate_groundtruth_sentiment(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have groundtruth sentiment labels on test tweets, we will then try various methods and train on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Let's try tweepy and TextBlob to add Sentiment to each Tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Let's use a tweepy and TextBlob to add Sentiment to each Tweet\n",
    "\n",
    "Two blogs that use tweepy and TextBlob can be found at \n",
    "- https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/analyze-tweet-sentiment-in-python/\n",
    "- https://medium.com/better-programming/twitter-sentiment-analysis-15d8892c0082\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. We compute Sentiment of Tweet using TextBlob\n",
    "- polarity is whether or not the tweet is positive, negative, or neutral ( scaled from 1 to -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We r going to use tweepy and TextBlob for tweets\n",
    "import tweepy as tw\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a function to get the subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return  TextBlob(text).sentiment.polarity\n",
    "\n",
    "# We eliminate words less then 3 characters long and standardize all words to lowercase\n",
    "def filter_words(words):\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "    return words_filtered\n",
    "\n",
    "\n",
    "# return neutral if small\n",
    "def calculateSentiment(text, neutralCutoff = 0.05):\n",
    "    polarity = getPolarity(text)\n",
    "    if abs(polarity) < neutralCutoff:\n",
    "        return 'neutral'\n",
    "    if polarity > 0:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = X_test.apply(calculateSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score is 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(f'accuracy_score is {accuracy_score(Y_test_groundtruth, y_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we get 0.6 accuracy by guessing the neutralCutoff = 0.05.  Let's try to see if we can do better by trying different neutralCutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestAccuracy is at 0.67 with neutralCutoff at 0.22\n"
     ]
    }
   ],
   "source": [
    "from numpy import arange\n",
    "\n",
    "bestCutoff = 0.05\n",
    "bestAccuracy = 0.6\n",
    "\n",
    "for i in arange(0.0, 0.5, 0.02):\n",
    "    y_test_i = X_test.apply(calculateSentiment, neutralCutoff=i)\n",
    "    accuracy_i = accuracy_score(Y_test_groundtruth, y_test_i)\n",
    "    if accuracy_i > bestAccuracy:\n",
    "        bestAccuracy = accuracy_i\n",
    "        bestCutoff = i\n",
    "\n",
    "print(f'bestAccuracy is at {bestAccuracy} with neutralCutoff at {bestCutoff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recalculate the y_test with best neutralCutoff at 0.22\n",
    "y_test = X_test.apply(calculateSentiment, neutralCutoff=0.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix is \n",
      "[[ 4 15  2]\n",
      " [ 2 48  4]\n",
      " [ 0 10 15]]\n"
     ]
    }
   ],
   "source": [
    "print(f'confusion matrix is \\n{confusion_matrix(Y_test_groundtruth, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.19      0.30        21\n",
      "     neutral       0.66      0.89      0.76        54\n",
      "    positive       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.67       100\n",
      "   macro avg       0.68      0.56      0.57       100\n",
      "weighted avg       0.67      0.67      0.63       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'classification report is \\n{classification_report(Y_test_groundtruth, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Let's try LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y = np.array([calculateSentiment(xi, neutralCutoff=0.22) for xi in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    ")\n",
    "features = vectorizer.fit_transform(\n",
    "    data['text']\n",
    ")\n",
    "features_nd = features.toarray() # for easy usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.twilio.com/blog/2017/12/sentiment-analysis-scikit-learn.html\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_logistic, X_test_logistic, y_train_logistic, y_test_logistic  = train_test_split(\n",
    "        features_nd, \n",
    "        y,\n",
    "        train_size=0.90, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = log_model.fit(X=X_train_logistic, y=y_train_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score is 0.46\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy_score is {accuracy_score(Y_test_groundtruth, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         6\n",
      "     neutral       0.74      0.86      0.80        73\n",
      "    positive       0.18      0.10      0.12        21\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.31      0.32      0.31       100\n",
      "weighted avg       0.58      0.65      0.61       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. We can try the NLTK toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'is', 'very', 'cute']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def format_sentence(sent):\n",
    "    return({word: True for word in nltk.word_tokenize(sent)})\n",
    "\n",
    "print(nltk.word_tokenize(\"The cat is very cute\"))\n",
    "\n",
    "def format_sentence_with_sentiment(sent):\n",
    "    formatted = format_sentence(sent)\n",
    "    sentiment = calculateSentiment(sent, neutralCutoff = 0.22)\n",
    "    return [formatted, sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Stars': True, 'are': True, 'great': True, '!': True}, 'positive']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_sentence_with_sentiment(\"Stars are great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Rats': True, 'smell': True, 'very': True, 'bad': True}, 'negative']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_sentence_with_sentiment(\"Rats smell very bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jc487/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                https://t.co/YHscjY6G8t\n",
       "1                                https://t.co/OLZnCJq93Y\n",
       "2                                https://t.co/cwOQLhQNFq\n",
       "3      RT @WhiteHouse: LIVE: President @realDonaldTru...\n",
       "4      ...Why won’t they do it, and why are they so f...\n",
       "                             ...                        \n",
       "995    RT @chefjclark: Amen ⁦@bkirkland7⁩! https://t....\n",
       "996      https://t.co/gsFSghkmdM https://t.co/zNoPFsTnn3\n",
       "997    Big problems and discrepancies with Mail In Ba...\n",
       "998    RT @realDonaldTrump: The Fake News Media is ri...\n",
       "999    RT @realDonaldTrump: Joe Biden called me Georg...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nltk = X.apply(format_sentence_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [{'https': True, ':': True, '//t.co/YHscjY6G8t...\n",
       "1    [{'https': True, ':': True, '//t.co/OLZnCJq93Y...\n",
       "2    [{'https': True, ':': True, '//t.co/cwOQLhQNFq...\n",
       "3    [{'RT': True, '@': True, 'WhiteHouse': True, '...\n",
       "4    [{'...': True, 'Why': True, 'won': True, '’': ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nltk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = X_nltk[:int((.9)*len(X_nltk))]\n",
    "test =  X_nltk[int((.1)*len( X_nltk)):] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     WIN = True           positi : neutra =     28.6 : 1.0\n",
      "                    Fake = True           negati : positi =     26.1 : 1.0\n",
      "                 Corrupt = True           negati : neutra =     25.0 : 1.0\n",
      "                decision = True           negati : neutra =     25.0 : 1.0\n",
      "                  things = True           negati : neutra =     25.0 : 1.0\n",
      "                   Great = True           positi : neutra =     24.4 : 1.0\n",
      "                   place = True           negati : neutra =     19.3 : 1.0\n",
      "                       * = True           negati : neutra =     17.9 : 1.0\n",
      "                  Andrew = True           negati : neutra =     17.9 : 1.0\n",
      "                     FOR = True           negati : neutra =     17.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "example1 = \"America is great\"\n",
    "\n",
    "print(classifier.classify(format_sentence(example1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "example1 = \"Mail in ballots are fraudelent\"\n",
    "\n",
    "print(classifier.classify(format_sentence(example1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45555555555555555\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the accuracy vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nltk_predict(sent):\n",
    "    return classifier.classify(format_sentence(sent))\n",
    "\n",
    "y_pred_nltk = X_test.apply(nltk_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score is 0.51\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy_score is {accuracy_score(Y_test_groundtruth, y_pred_nltk)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.28      0.81      0.42        21\n",
      "     neutral       0.92      0.41      0.56        54\n",
      "    positive       0.75      0.48      0.59        25\n",
      "\n",
      "    accuracy                           0.51       100\n",
      "   macro avg       0.65      0.57      0.52       100\n",
      "weighted avg       0.74      0.51      0.54       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test_groundtruth, y_pred_nltk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. We train a FaceBook FastText model to build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fasttext_record(sent):\n",
    "    sentiment = calculateSentiment(sent, neutralCutoff = 0.22)\n",
    "    return f'__label__{sentiment} {sent}'\n",
    "\n",
    "def write_fasttext_sentiment_to_file(sent, fileName):\n",
    "    file = open(fileName, \"a+\")  \n",
    "    record = create_fasttext_record(sent)\n",
    "    file.write(f'{record}\\n')\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import clean_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96     None\n",
       "471    None\n",
       "312    None\n",
       "449    None\n",
       "882    None\n",
       "       ... \n",
       "759    None\n",
       "861    None\n",
       "42     None\n",
       "555    None\n",
       "367    None\n",
       "Name: text, Length: 630, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFile = \"trumpsentiment.train\"\n",
    "\n",
    "clean_file(trainFile)\n",
    "X_train.apply(write_fasttext_sentiment_to_file, fileName=trainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875    None\n",
       "377    None\n",
       "412    None\n",
       "274    None\n",
       "763    None\n",
       "       ... \n",
       "346    None\n",
       "282    None\n",
       "323    None\n",
       "803    None\n",
       "58     None\n",
       "Name: text, Length: 270, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationFile = \"trumpsentiment.valid\"\n",
    "clean_file(validationFile)\n",
    "X_val.apply(write_fasttext_sentiment_to_file, fileName=validationFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357    None\n",
       "81     None\n",
       "436    None\n",
       "657    None\n",
       "654    None\n",
       "       ... \n",
       "742    None\n",
       "34     None\n",
       "132    None\n",
       "530    None\n",
       "886    None\n",
       "Name: text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFile = \"trumpsentiment.test\"\n",
    "clean_file(testFile)\n",
    "X_test.apply(write_fasttext_sentiment_to_file, fileName=testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=trainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__neutral',), array([0.90248114]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Trump will is awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 0.6666666666666666, 0.6666666666666666)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.test(validationFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 0.73, 0.73)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.test(testFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on validation with 270 records, the precision is 0.666666 and the recall is 0.6666\n",
    "## Performance on the test with 100 records, the precision is 0.73 and the recall is 0.73\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hmm, let's try to do better with hyperparameter tuning.  FastText has various hyperparameters such as epochs, learning rate, and ngrams.  This made available to us via trial and error or we can try the autotune feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_autotuned = fasttext.train_supervised(input=trainFile, autotuneValidationFile=validationFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 0.7518518518518519, 0.7518518518518519)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_autotuned.test(validationFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 0.73, 0.73)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_autotuned.test(testFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So even though we got better performance on the validation file with precision/recall at 0.75, the performance was the same on the testFile at 0.73.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For future work, we could try to clean the twitter text to get better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import clean_twitter_text"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPW+nXq5kNmMD0i1KQMwvga",
   "include_colab_link": true,
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (dsc540)",
   "language": "python",
   "name": "dsc540"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
